<!doctype html><html lang=en dir=auto data-theme=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Atten-hut! | Arav Tewari's Blog</title><meta name=keywords content><meta name=description content="In this blog, I want to build attention from scratch. But I don&rsquo;t want to jump straight into the best implementation. We&rsquo;re going to start with dumbest, most naive possible code, see why it fails, and then add one idea at a time until attention falls on your head like the apple fell on Newton&rsquo;s.
I learned it this way and it really stuck to me. This is heavily inspired from Karpathy&rsquo;s video, and this concept has probably been beaten to hell. But the point of this blog is to just practice formalizing my notes."><meta name=author content="Arav Tewari"><link rel=canonical href=http://localhost:1313/posts/attention/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/attention/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="http://localhost:1313/posts/attention/"><meta property="og:site_name" content="Arav Tewari's Blog"><meta property="og:title" content="Atten-hut!"><meta property="og:description" content="In this blog, I want to build attention from scratch. But I don’t want to jump straight into the best implementation. We’re going to start with dumbest, most naive possible code, see why it fails, and then add one idea at a time until attention falls on your head like the apple fell on Newton’s.
I learned it this way and it really stuck to me. This is heavily inspired from Karpathy’s video, and this concept has probably been beaten to hell. But the point of this blog is to just practice formalizing my notes."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-08T19:11:45-05:00"><meta property="article:modified_time" content="2026-02-08T19:11:45-05:00"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Atten-hut!"><meta name=twitter:description content="In this blog, I want to build attention from scratch. But I don&rsquo;t want to jump straight into the best implementation. We&rsquo;re going to start with dumbest, most naive possible code, see why it fails, and then add one idea at a time until attention falls on your head like the apple fell on Newton&rsquo;s.
I learned it this way and it really stuck to me. This is heavily inspired from Karpathy&rsquo;s video, and this concept has probably been beaten to hell. But the point of this blog is to just practice formalizing my notes."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Atten-hut!","item":"http://localhost:1313/posts/attention/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Atten-hut!","name":"Atten-hut!","description":"In this blog, I want to build attention from scratch. But I don\u0026rsquo;t want to jump straight into the best implementation. We\u0026rsquo;re going to start with dumbest, most naive possible code, see why it fails, and then add one idea at a time until attention falls on your head like the apple fell on Newton\u0026rsquo;s.\nI learned it this way and it really stuck to me. This is heavily inspired from Karpathy\u0026rsquo;s video, and this concept has probably been beaten to hell. But the point of this blog is to just practice formalizing my notes.\n","keywords":[],"articleBody":"In this blog, I want to build attention from scratch. But I don’t want to jump straight into the best implementation. We’re going to start with dumbest, most naive possible code, see why it fails, and then add one idea at a time until attention falls on your head like the apple fell on Newton’s.\nI learned it this way and it really stuck to me. This is heavily inspired from Karpathy’s video, and this concept has probably been beaten to hell. But the point of this blog is to just practice formalizing my notes.\nThe task: next character prediction The setup is: given a set of characters, how can we predict the next best character? Models like ChatGPT don’t actually work at the character level, they work at something called the token level. This is a blog for later, but tokens are just parts of a word. So a word like “homework” will be broken into 2 tokens “home-work”. For now, our tokens are characters. The set of unique tokens is called our vocabulary, and it is of size $V$.\nSo, we take long string (more formally called a sequence) of text, convert it to integers, and try to train a model that predicts the next character given the previous ones.\nIf the sequence of tokens is $$ (x_1, x_2, \\dots, x_T) $$ then at position $t$, the model sees $x_1, x_2, \\dots, x_t$ and is trained to predict $x_{t+1}$. This can be written as: $$ p(x_{t+1} \\mid x_1, \\dots, x_t) $$What’s important to note here is how many tokens can the model see into the past? How big should $t$ be? That’s called context. The code below clearly visualizes that idea.\nblock_size = 8 x = train_data[:block_size] y = train_data[1:block_size + 1] for t in range(block_size): context = x[:t + 1] target = y[t] print(f'When context is {context}, target is {target}') When context is [18], target is 47 When context is [18, 47], target is 56 When context is [18, 47, 56], target is 57 When context is [18, 47, 56, 57], target is 58 When context is [18, 47, 56, 57, 58], target is 1 When context is [18, 47, 56, 57, 58, 1], target is 15 When context is [18, 47, 56, 57, 58, 1, 15], target is 47 When context is [18, 47, 56, 57, 58, 1, 15, 47], target is 58 block_size is how big our context is. We look 8 tokens into the past to predict the next one. x (our input) is the first 8 samples in our train set. y (our output or target) is also 8 samples long, but it’s offset by 1. Why the offset? It’s easier to answer that with an example.\nAt timestep 0, we want to use the all tokens up to token 0 (so x = [token_0]) to predict y = token_1.\nAt timestep 1, we we want to use all tokens up to token 1 (so x = [token_0, token_1]) to predict y = token_2.\nAt timestep 3, we want to use all tokens up to token 2 (so x = [token_0, token_1, token_2]) to predict the y = token_3.\nExtend this to some timestep t, we get “we want to use all tokens up to token t to predict the t+1 token. And if we make y by an offset by one, token t+1 is stored at index t. Now the code should make sense!\nThe simplest possible model Now that we understand what we are training our model to do, let’s start with the easiest implementation. This would be to just ignore the context and only look at the last token when predicting the current one. This is called a bigram language model.\nThe way we implement this is a lookup table. We should be able to look up the current token and get scores for the next token, and we choose the token with the highest score. So this table should have $V$ spots (remember $V$ is the size of our vocabulary or the number of unique tokens). And in each spot, we should have a score for all of the tokens, which means we should have $V$ scores. This means our table will be size $V \\times V$.\nWe can code this out as follows.\ntoken_table = nn.Embedding(vocab_size, vocab_size) This creates a matrix: $$ W \\in \\R ^ {V \\times V} $$ Given an input token $x_t \\in \\{0, \\dots, V-1 \\}$, the table returns logits $$ \\ell _t = W[x_t] \\in \\R^V $$ The logits are our scores for each token. We can convert these scores into a probability distribution, and then sample from that. That will give us our next, most likely token.\nThe way we do that is by passing the logits through a function called the softmax. This will take our raw scores and scale each value to be between 0 and 1 and make sure all the values sum up to 1. This is the requirement for a probability distribution. Finally, we sample from this distribution to get our next token.\nIn PyTorch, this is coded as:\nclass BigramModel(nn.Module): def __init__(self, vocab_size): self.token_table = nn.Embedding(vocab_size, vocab_size) def forward(self, current_token): next_token_logits = self.token_table(current_token) return next_token_logits def generate(self, current_token, number_of_new_tokens): result = [] for _ in range(number_of_new_tokens): # make prediction next_token_logits = self.forward(current_token) # convert from probability distribution next_token_dist = F.softmax(next_token_logits, dim=-1) # sample from distribution next_token = torch.multinomial(next_token_dist, num_samples=1) # append next token to result result.append(next_token) # move onto next time step curr_token = next_token return result Right now, this simple bigram model doesn’t use any of the previous tokens to inform its predictions. Formally, we are learning $$ p(x_{t+1} \\mid x_t) $$ but our task requires learning $$ p(x_{t+1} \\mid x_1, \\dots, x_t) $$The mathematical trick in attention How can we use the information from our context. Well, the first thing we need to do is assume our tokens contain information. Instead of simple scalars, we can say our tokens are vectors of size $C$, which stands for channels. These fancier tokens are called token embeddings. So suppose our scalar tokens $x_1, x_2, \\dots x_T \\in \\R$ have been embedded into a vector space and are now $$ x_1, x_2, \\dots, x_T \\in \\R ^C $$The simplest way we can use our context is by looking at the previous tokens and take their average, which would represent some sort of feature vector. Note that taking the average is pretty lossy because we lose quite a bit of information (like each tokens position and we treat all tokens equally). We will see on how to recover that later. So we want $$ \\bar{x}_t = \\frac{1}{t} \\sum_{i=1}^t x_i $$ This is pretty simple to code out. Assume that we have a sequence of 8 tokens and each token is in a 2D vector space:\nT, C = 8, 2 # time, channels x = torch.randn(T, C) # dummy input x_bar = torch.zeros((T, C)) for t in range(T): prev_context = x[:t+1] # get all tokens up to timestep t x_bar[t] = torch.mean(prev_context, 0) # take the average print(x_bar[0] == x[0]) # true, because the average only consists of the first token print(x_bar[1] == x[1]) # false, because the average now consists of the first 2 tokens Finding the average from a for loop is a pretty slow operation. Let’s see how to speed this up.\nWe can think of the average as a weighted sum. For example, if we are taking the average of 4 inputs, then we are multiplying each input by 0.25 and adding them up. This is literally the summation expanded out $$ \\bar{x}_4 = \\frac{1}{4} \\sum_{i=1}^4 x_i \\\\ \\bar{x}_4 = \\frac{1}{4} (x_1 + x_2 + x_3 + x_4) \\\\ \\bar{x}_4 = \\frac{1}{4} x_1 + \\frac{1}{4} x_2 + \\frac{1}{4} x_3 + \\frac{1}{4} x_4 $$We can express this weighted sum as a dot product between two vectors: the inputs and their respective weights. Suppose each $x_i \\in \\R^C$ is a vector (not a scalar). Then the average at timestep 4 can be written as $$ \\bar{x}_4 = \\begin{bmatrix} \\frac{1}{4} \u0026 \\frac{1}{4} \u0026 \\frac{1}{4} \u0026 \\frac{1}{4} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{bmatrix} $$The first vector is our weights and the second transposed vector is our input.\nNow remember in each time step, we have one more token added to our context when predicting the next target token.\nt=0: When context is [x1], target is x2 t=1: When context is [x1, x2], target is x3 t=2: When context is [x1, x2, x3], target is x4 At timestep $t = 0$, the context is just $[x_1]$, so the average is $$ \\bar{x}_1 = 1 \\cdot x_1 $$At timestep $t=1$, the context is $[x_1, x_2]$, so the average is $$ \\bar{x}_2 = \\frac{1}{2} x_1 + \\frac{1}{2} x_2 $$At timestep $t=2$, the context is $[x_1, x_2, x_3]$, so the average is $$ \\bar{x}_3 = \\frac{1}{3} x_1 + \\frac{1}{3} x_2 + \\frac{1}{3} x_3 $$And remember how we just reformulated the average into a dot product between the inputs and the weights? The weights at each timstep would clearly look like $$ [1] $$ $$ [\\tfrac{1}{2}, \\, \\tfrac{1}{2}] $$ $$ \\left[\\tfrac{1}{3}, \\, \\tfrac{1}{3}, \\, \\tfrac{1}{3}\\right] $$Now with these two sets of input and weight vectors, we can arrange them into matrices. Let the input matrix be $$ X = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\in \\mathbb{R}^{3 \\times C}. $$And let the weight matrix be $$ W = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\[3px] \\frac{1}{2} \u0026 \\frac{1}{2} \u0026 0 \\\\[3px] \\frac{1}{3} \u0026 \\frac{1}{3} \u0026 \\frac{1}{3} \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 3}. $$Notice that $W$ is lower triangular. That is important. It ensures that at timestep $t$, only tokens up to $t$ are used in the average If we are predicting token 5, we should only be averaging tokens 1-4 from the data. Using information from the actual token 5 in the data or tokens 6, 7, $\\dots T$ is cheating! The zeros prevent any future token from leaking into the computation. This is called the causal mask.\nAnd to perform the dot product and get the averaged out tokens, we just do a matrix multiplication: $$ \\bar{X} = W X \\in \\R^{3 \\times C} $$The result contains $$ \\bar{X} = \\begin{bmatrix} \\bar{x}_1 \\\\ \\bar{x}_2 \\\\ \\bar{x}_3 \\end{bmatrix} $$Each row of $\\bar{X}$ is the average of all tokens up to that timestep.\nThat’s the mathematical trick in attention! Instead of writing for-loops that repeatedly sum over previous tokens, the entire operation can be expressed as a single matrix multiplication that acts as a dot product between weights and the inputs.\nThis can be coded very easily\nT, C = 8, 2 # time, channels x = torch.randn(T, C) # same dummy input # create weights matrix for weighted sum weights = torch.ones(T, T) # all 1s matrix weights = torch.tril(weights) # now upper triangle is 0s and only lower triangle is 1s weights = weights / torch.sum(weights, dim=1, keepdims=True) # have each row sum up to 1 # average out tokens via matrix multiply xbar2 = weights @ x Again, we start with the same dummy input where the sequence length is 8 and each token is in a 2D vector space. Then we instantiate the weights matrix $W$ of shape $T \\times T$ where every value is 1. $$ W = \\begin{bmatrix} 1 \u0026 1 \u0026 1 \\\\[3px] 1 \u0026 1 \u0026 1 \\\\[3px] 1 \u0026 1 \u0026 1 \\end{bmatrix} $$We pass this matrix into torch.tril(), which is a nifty method returns the same matrix but the upper triangle is made all 0. $$ W = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\[3px] 1 \u0026 1 \u0026 0 \\\\[3px] 1 \u0026 1 \u0026 1 \\end{bmatrix} $$Then we normalize each row to sum up to 1 by finding the total sum along each row (dim=1) and dividing each element in that row by the sum. $$ W = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\[3px] \\frac{1}{2} \u0026 \\frac{1}{2} \u0026 0 \\\\[3px] \\frac{1}{3} \u0026 \\frac{1}{3} \u0026 \\frac{1}{3} \\end{bmatrix} $$Finally, we take the average by executing the matrix multiplication between the weights and the inputs.\nAdding softmax Notice how in the last step we were normalizing the matrix so that each row sums up to 1. The keen reader will realize that we can use softmax to do that for us.\nRight now, our matrix before the normalization looks like $$ W = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\[3px] 1 \u0026 1 \u0026 0 \\\\[3px] 1 \u0026 1 \u0026 1 \\end{bmatrix} $$ For softmax to generate our desired normalized output, we need to input something like $$ W = \\begin{bmatrix} 0 \u0026 -\\infty \u0026 -\\infty \\\\[3px] 0 \u0026 0 \u0026 -\\infty \\\\[3px] 0 \u0026 0 \u0026 0 \\end{bmatrix} $$This is because softmax is defined as $$ \\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}. $$ So if an entry is $-\\infty$, then $e^{-\\infty} = 0$, which means those positions get probability 0. And if all the unmasked entries are 0, softmax makes them uniform.\nIn code, this would look like\nT, C = 8, 2 # time, channels x = torch.randn(T, C) # same dummy input tril = torch.tril(torch.ones(T, T)) weights = torch.zeros((T,T)) weights = weights.masked_fill(tril == 0, float('-inf')) print(weights) print(\"---\") weights = F.softmax(weights, dim=1) # normalize print(weights) xbar3 = weights @ x tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf], [0., 0., -inf, -inf, -inf, -inf, -inf, -inf], [0., 0., 0., -inf, -inf, -inf, -inf, -inf], [0., 0., 0., 0., -inf, -inf, -inf, -inf], [0., 0., 0., 0., 0., -inf, -inf, -inf], [0., 0., 0., 0., 0., 0., -inf, -inf], [0., 0., 0., 0., 0., 0., 0., -inf], [0., 0., 0., 0., 0., 0., 0., 0.]]) --- tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000], [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000], [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000], [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000], [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]]) We create a tril matrix where the upper triangle is all zeros, and a weights matrix of the same shape but the entire thing is zeros. Then we lay the tril matrix on the weights one, and wherever tril is 0 (the upper triangle half), we change that spot in weights to $- \\infty$. Then we apply softmax to the weights to normalize like before. And finally, we matrix multiply that with the input x to get our averaged scores of previous tokens.\nQueries, Keys, and Values Ok, this is good and all, but a token shouldn’t have to give equal weightage to all of its previous tokens. It should be able to pay special attention (hint hint!) to certain tokens that have more relevant information to offer. For example, if I am a token “dog”, I should pay more attention to tokens that describe me like “fluffy” or “running” instead of other tokens that don’t matter as much–it’s data dependent.\nHow do we implement that? Imagine each token emits two vectors: a query and key. These are different metadata about itself. The query vector, roughly speaking, asks the question “What am I looking for?” The key vector says “This is what information I contain.”\nFormally, each token has an embedding $x_t \\in \\R ^C$. We want to transform these vectors from dimension $C$ to another $d_k$. If you remember from linear algebra, that’s exactly what a matrix does: transform a vector from one dimension to a vector in another dimension by linearly projecting it. So we learn two linear projections $$ W_Q \\in \\R^{C \\times d_k}, \\quad W_K \\in \\R^{C \\times d_k} $$To get our query and key metadata for a token, we comput $$ q_t = x_t W_Q, \\quad k_t = x_t W_K $$ where $q_t, k_t \\in \\R ^{d_k}$ (just fancy notation saying that the vectors are now in another dimension).\nIf we stack all token embeddings into a matrix $$ X \\in \\R ^{T \\times C}, $$ then we get $$ Q = X W_Q, \\quad K = X W_K \\in \\R ^ {T \\times d_k} $$Implementing this in code is very easy. We can use torch.nn.Linear() to make the learnable projections:\nT, C = 8, 32 # time, bigger channels now x = torch.randn(T, C) # dummy input shape (T, C) head_size = 16 # this is our d_k # these linear layers (simple matmuls) are what we use to emit the key and query vectors key = nn.Linear(C, head_size, bias=False) query = nn.Linear(C, head_size, bias=False) k = key(x) # shape (T, 16) q = query(x) # shape (T, 16) Now to have each token communicate with each other, we take the dot product ($Q \\cdot K)$. Each token’s query vector will be multiplied by every other token’s key vector. If a key and query vector are aligned, then their dot product will be very high and that’s how we know they are related.\nFormally, we compute the following attention score matrix $$ S = QK ^\\top \\in \\R ^{T \\times T}, $$ where $S_{t, i} = q_t \\cdot k_i$.\nRow $t$ of $S$ contains how much token $t$ is interested in every other token. And this score matrix is what replaces our initial average score matrix! Now each weight can be custom tuned to how much a token should pay attention to other tokens.\n# Up to this point, NO communication has happened yet between the tokens. # We've only computed their key and query vectors. # The dot product below is when the communication happens! weights = q @ k.T # (T, 16) @ (16, T) -\u003e (T, T) From here, we take our scores matrix and apply softmax like before.\ntril = torch.tril(torch.ones(T, T)) weights = weights.masked_fill(tril == 0, float('-inf')) weights = F.softmax(weights, dim=1) tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000], [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000], [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000], [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000], [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]] Notice how the weights are not even now across each row! Tokens are paying more attention to some tokens than others and giving them higher scores.\nFinally, we don’t aggregate across the raw input x like before. Instead, we calculate another vector from the input, called value. We aggregate across this. The value vector says, “If you find me interesting (aka we have a high $Q \\cdot K$ dot product), this is what I can communicate to you.”\nFormally, we learn a third projection $$ W_V \\in \\R ^ {C \\times d_v}, $$ and compute $$ v_t = x_t W_v, \\quad V=X W_V \\in \\R ^ {T \\times d_v} $$value = nn.Linear(C, head_size, bias=False) # our d_v == d_k v = value(x) out = weights @ v Minor notes A few things to note:\nKeys and queries are projected to $d_k$ and values are projected to $d_v$. In this case, $d_k = d_v$. Attention is simply a communication mechanism. You can view it as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all its neighbors, where the weights are data dependent. Attention simply aggregates over a set of vectors–there is no notion of space or position. This is why we need to add positional encodings to the vectors. The tril() creates something called the causal mask, which prevents all tokens from seeing and communicating each other. Sometimes, this is important like in a task like sentiment classification. In this task, we need to see the entire sentence before we classify it as happy or sad. In such tasks, we can implement attention the same way, just without the mask. Self-attention just means that the keys and values come from the same source as the queries. Cross-attention means that the queries still get produced from input x, but the keys and values come from somewhere else (like an encoder block). Lastly, we can’t just use weights as is. Remember, weights is calculated by taking the dot product between $K$ and $Q$. Sometimes, these dot products can be really high and cause numerical instability. So, we need to control the variance of the matrix by scaling by $\\frac{1}{\\sqrt{d_k}}$. This way, the softmax output will remain fairly diffused and not too saturated (think of a really peaky probability distribution versus a fairly smooth one). Fin Finally, we have implemented the simplest form of attention: scaled dot product attention (SDPA for short)! $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right)V $$In the next blog post, we will see how the attention operation fits into a bigger model architecture called the transformer. We will compare the more powerful transformer against our humble bigram model and measure the difference in performance. In the end, we will actually have a model that can decently generate text!\n","wordCount":"3502","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2026-02-08T19:11:45-05:00","dateModified":"2026-02-08T19:11:45-05:00","author":{"@type":"Person","name":"Arav Tewari"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/attention/"},"publisher":{"@type":"Organization","name":"Arav Tewari's Blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css integrity=sha384-WcoG4HRXMzYzfCgiyfrySxx90XSl2rxY5mnVY5TwtWE6KLrArNKn0T/mOgNL0Mmi crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.js integrity=sha384-J+9dG2KMoiR9hqcFao0IBLwxt6zpcyN68IgwzsCSkbreXUjmNVRhPFTssqdSGjwQ crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Arav Tewari's Blog (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Arav Tewari's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://example.org title=Posts><span>Posts</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=http://localhost:1313/archive/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Atten-hut!</h1><div class=post-meta><span title='2026-02-08 19:11:45 -0500 EST'>February 8, 2026</span>&nbsp;·&nbsp;<span>17 min</span>&nbsp;·&nbsp;<span>Arav Tewari</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#the-task-next-character-prediction>The task: next character prediction</a></li><li><a href=#the-simplest-possible-model>The simplest possible model</a></li><li><a href=#the-mathematical-trick-in-attention>The mathematical trick in attention</a></li><li><a href=#adding-softmax>Adding softmax</a></li><li><a href=#queries-keys-and-values>Queries, Keys, and Values</a></li><li><a href=#minor-notes>Minor notes</a></li><li><a href=#fin>Fin</a></li></ul></nav></div></details></div><div class=post-content><p>In this blog, I want to build attention from scratch. But I don&rsquo;t want to jump straight into the best implementation. We&rsquo;re going to start with dumbest, most naive possible code, see why it fails, and then add one idea at a time until attention falls on your head like the apple fell on Newton&rsquo;s.</p><p>I learned it this way and it really stuck to me. This is heavily inspired from Karpathy&rsquo;s video, and this concept has probably been beaten to hell. But the point of this blog is to just practice formalizing my notes.</p><h2 id=the-task-next-character-prediction>The task: next character prediction<a hidden class=anchor aria-hidden=true href=#the-task-next-character-prediction>#</a></h2><p>The setup is: given a set of characters, how can we predict the next best character? Models like ChatGPT don&rsquo;t actually work at the character level, they work at something called the <strong>token</strong> level. This is a blog for later, but tokens are just parts of a word. So a word like &ldquo;homework&rdquo; will be broken into 2 tokens &ldquo;home-work&rdquo;. For now, our tokens are characters.
The set of unique tokens is called our <strong>vocabulary</strong>, and it is of size $V$.</p><p>So, we take long string (more formally called a <strong>sequence</strong>) of text, convert it to integers, and try to train a model that predicts the next character given the previous ones.</p><p>If the sequence of tokens is</p>$$
(x_1, x_2, \dots, x_T)
$$<p>then at position $t$, the model sees $x_1, x_2, \dots, x_t$ and is trained to predict $x_{t+1}$. This can be written as:</p>$$
p(x_{t+1} \mid x_1, \dots, x_t)
$$<p>What&rsquo;s important to note here is how many tokens can the model see into the past? How big should $t$ be? That&rsquo;s called <strong>context</strong>. The code below clearly visualizes that idea.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>block_size</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>train_data</span><span class=p>[:</span><span class=n>block_size</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>train_data</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=n>block_size</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>block_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>context</span> <span class=o>=</span> <span class=n>x</span><span class=p>[:</span><span class=n>t</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>target</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=n>t</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;When context is </span><span class=si>{</span><span class=n>context</span><span class=si>}</span><span class=s1>, target is </span><span class=si>{</span><span class=n>target</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span></code></pre></div><pre tabindex=0><code>When context is [18], target is 47
When context is [18, 47], target is 56
When context is [18, 47, 56], target is 57
When context is [18, 47, 56, 57], target is 58
When context is [18, 47, 56, 57, 58], target is 1
When context is [18, 47, 56, 57, 58,  1], target is 15
When context is [18, 47, 56, 57, 58,  1, 15], target is 47
When context is [18, 47, 56, 57, 58,  1, 15, 47], target is 58
</code></pre><p><code>block_size</code> is how big our context is. We look 8 tokens into the past to predict the next one. <code>x</code> (our input) is the first 8 samples in our train set. <code>y</code> (our output or target) is also 8 samples long, but it&rsquo;s offset by 1. Why the offset? It&rsquo;s easier to answer that with an example.</p><p>At timestep 0, we want to use the all tokens up to token 0 (so <code>x = [token_0]</code>) to predict <code>y = token_1</code>.</p><p>At timestep 1, we we want to use all tokens up to token 1 (so <code>x = [token_0, token_1]</code>) to predict <code>y = token_2</code>.</p><p>At timestep 3, we want to use all tokens up to token 2 (so <code>x = [token_0, token_1, token_2]</code>) to predict the <code>y = token_3</code>.</p><p>Extend this to some timestep <code>t</code>, we get &ldquo;we want to use all tokens up to token <code>t</code> to predict the <code>t+1</code> token. And if we make <code>y</code> by an offset by one, token <code>t+1</code> is stored at index <code>t</code>. Now the code should make sense!</p><h2 id=the-simplest-possible-model>The simplest possible model<a hidden class=anchor aria-hidden=true href=#the-simplest-possible-model>#</a></h2><p>Now that we understand what we are training our model to do, let&rsquo;s start with the easiest implementation. This would be to just ignore the context and only look at the last token when predicting the current one. This is called a bigram language model.</p><p>The way we implement this is a <strong>lookup table</strong>. We should be able to look up the current token and get <strong>scores</strong> for the next token, and we choose the token with the highest score. So this table should have $V$ spots (remember $V$ is the size of our vocabulary or the number of unique tokens). And in each spot, we should have a score for all of the tokens, which means we should have $V$ scores. This means our table will be size $V \times V$.</p><p>We can code this out as follows.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>token_table</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>)</span>
</span></span></code></pre></div><p>This creates a matrix:</p>$$
W \in \R ^ {V \times V}
$$<p>Given an input token $x_t \in \{0, \dots, V-1 \}$, the table returns <strong>logits</strong></p>$$
\ell _t = W[x_t] \in \R^V
$$<p>The logits are our scores for each token. We can convert these scores into a probability distribution, and then sample from that. That will give us our next, most likely token.</p><p>The way we do that is by passing the logits through a function called the <strong>softmax</strong>. This will take our raw scores and scale each value to be between 0 and 1 and make sure all the values sum up to 1. This is the requirement for a probability distribution. Finally, we sample from this distribution to get our next token.</p><p>In PyTorch, this is coded as:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>BigramModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>token_table</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>current_token</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>next_token_logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>token_table</span><span class=p>(</span><span class=n>current_token</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>next_token_logits</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>generate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>current_token</span><span class=p>,</span> <span class=n>number_of_new_tokens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>result</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>number_of_new_tokens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=c1># make prediction</span>
</span></span><span class=line><span class=cl>            <span class=n>next_token_logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>current_token</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># convert from probability distribution</span>
</span></span><span class=line><span class=cl>            <span class=n>next_token_dist</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>next_token_logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># sample from distribution</span>
</span></span><span class=line><span class=cl>            <span class=n>next_token</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>multinomial</span><span class=p>(</span><span class=n>next_token_dist</span><span class=p>,</span> <span class=n>num_samples</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># append next token to result</span>
</span></span><span class=line><span class=cl>            <span class=n>result</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>next_token</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># move onto next time step</span>
</span></span><span class=line><span class=cl>            <span class=n>curr_token</span> <span class=o>=</span> <span class=n>next_token</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>result</span>
</span></span></code></pre></div><p>Right now, this simple bigram model doesn&rsquo;t use any of the previous tokens to inform its predictions. Formally, we are learning</p>$$
p(x_{t+1} \mid x_t)
$$<p>but our task requires learning</p>$$
p(x_{t+1} \mid x_1, \dots, x_t)
$$<h2 id=the-mathematical-trick-in-attention>The mathematical trick in attention<a hidden class=anchor aria-hidden=true href=#the-mathematical-trick-in-attention>#</a></h2><p>How can we use the information from our context. Well, the first thing we need to do is assume our tokens contain information. Instead of simple scalars, we can say our tokens are vectors of size $C$, which stands for channels. These fancier tokens are called <strong>token embeddings.</strong> So suppose our scalar tokens $x_1, x_2, \dots x_T \in \R$ have been embedded into a vector space and are now</p>$$
x_1, x_2, \dots, x_T \in \R ^C
$$<p>The simplest way we can use our context is by looking at the previous tokens and <strong>take their average</strong>, which would represent some sort of feature vector. Note that taking the average is pretty lossy because we lose quite a bit of information (like each tokens position and we treat all tokens equally). We will see on how to recover that later. So we want</p>$$
\bar{x}_t = \frac{1}{t} \sum_{i=1}^t x_i
$$<p>This is pretty simple to code out. Assume that we have a sequence of 8 tokens and each token is in a 2D vector space:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>T</span><span class=p>,</span> <span class=n>C</span> <span class=o>=</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>2</span> <span class=c1># time, channels</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>T</span><span class=p>,</span> <span class=n>C</span><span class=p>)</span> <span class=c1># dummy input</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x_bar</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>T</span><span class=p>,</span> <span class=n>C</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>T</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>prev_context</span> <span class=o>=</span> <span class=n>x</span><span class=p>[:</span><span class=n>t</span><span class=o>+</span><span class=mi>1</span><span class=p>]</span> <span class=c1># get all tokens up to timestep t</span>
</span></span><span class=line><span class=cl>    <span class=n>x_bar</span><span class=p>[</span><span class=n>t</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>prev_context</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=c1># take the average</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x_bar</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>==</span> <span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span> <span class=c1># true, because the average only consists of the first token</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x_bar</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>==</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span> <span class=c1># false, because the average now consists of the first 2 tokens</span>
</span></span></code></pre></div><p>Finding the average from a for loop is a pretty slow operation. Let&rsquo;s see how to speed this up.</p><p>We can think of the average as a <strong>weighted sum</strong>. For example, if we are taking the average of 4 inputs, then we are multiplying each input by 0.25 and adding them up. This is literally the summation expanded out</p>$$
\bar{x}_4 = \frac{1}{4} \sum_{i=1}^4 x_i \\
\bar{x}_4 = \frac{1}{4} (x_1 + x_2 + x_3 + x_4) \\
\bar{x}_4 = \frac{1}{4} x_1 + \frac{1}{4} x_2 + \frac{1}{4} x_3 + \frac{1}{4} x_4
$$<p>We can express this weighted sum as a <strong>dot product</strong> between two vectors: the inputs and their respective weights. Suppose each $x_i \in \R^C$ is a vector (not a scalar). Then the average at timestep 4 can be written as</p>$$
\bar{x}_4 =
\begin{bmatrix}
\frac{1}{4} & \frac{1}{4} & \frac{1}{4} & \frac{1}{4}
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4
\end{bmatrix}
$$<p>The first vector is our weights and the second transposed vector is our input.</p><p>Now remember in each time step, we have one more token added to our context when predicting the next target token.</p><pre tabindex=0><code>t=0: When context is [x1], target is x2
t=1: When context is [x1, x2], target is x3
t=2: When context is [x1, x2, x3], target is x4
</code></pre><p>At timestep $t = 0$, the context is just $[x_1]$, so the average is</p>$$
\bar{x}_1 = 1 \cdot x_1
$$<p>At timestep $t=1$, the context is $[x_1, x_2]$, so the average is</p>$$
\bar{x}_2 = \frac{1}{2} x_1 + \frac{1}{2} x_2
$$<p>At timestep $t=2$, the context is $[x_1, x_2, x_3]$, so the average is</p>$$
\bar{x}_3 = \frac{1}{3} x_1 + \frac{1}{3} x_2 + \frac{1}{3} x_3
$$<p>And remember how we just reformulated the average into a dot product between the inputs and the weights? The weights at each timstep would clearly look like</p>$$
[1]
$$<p></p>$$
[\tfrac{1}{2}, \, \tfrac{1}{2}]
$$<p></p>$$
\left[\tfrac{1}{3}, \, \tfrac{1}{3}, \, \tfrac{1}{3}\right]
$$<p>Now with these two sets of input and weight vectors, we can arrange them into matrices.
Let the input matrix be</p>$$
X =
\begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix}
\in \mathbb{R}^{3 \times C}.
$$<p>And let the weight matrix be</p>$$
W =
\begin{bmatrix}
1 & 0 & 0 \\[3px]
\frac{1}{2} & \frac{1}{2} & 0 \\[3px]
\frac{1}{3} & \frac{1}{3} & \frac{1}{3}
\end{bmatrix}
\in \mathbb{R}^{3 \times 3}.
$$<p>Notice that $W$ is lower triangular. That is important. It ensures that at timestep $t$, only tokens up to $t$ are used in the average If we are predicting token 5, we should only be averaging tokens 1-4 from the data. Using information from the actual token 5 in the data or tokens 6, 7, $\dots T$ is cheating! The zeros prevent any <em>future</em> token from leaking into the computation. This is called the <strong>causal mask</strong>.</p><p>And to perform the dot product and get the averaged out tokens, we just do a <strong>matrix multiplication</strong>:</p>$$
\bar{X} = W X \in \R^{3 \times C}
$$<p>The result contains</p>$$
\bar{X} =
\begin{bmatrix}
\bar{x}_1 \\
\bar{x}_2 \\
\bar{x}_3
\end{bmatrix}
$$<p>Each row of $\bar{X}$ is the average of all tokens up to that timestep.</p><p>That&rsquo;s the mathematical trick in attention! Instead of writing for-loops that repeatedly sum over previous tokens, the entire operation can be expressed as a single matrix multiplication that acts as a dot product between weights and the inputs.</p><p>This can be coded very easily</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>T</span><span class=p>,</span> <span class=n>C</span> <span class=o>=</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>2</span> <span class=c1># time, channels</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>T</span><span class=p>,</span> <span class=n>C</span><span class=p>)</span> <span class=c1># same dummy input</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># create weights matrix for weighted sum</span>
</span></span><span class=line><span class=cl><span class=n>weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>T</span><span class=p>,</span> <span class=n>T</span><span class=p>)</span> <span class=c1># all 1s matrix</span>
</span></span><span class=line><span class=cl><span class=n>weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>weights</span><span class=p>)</span> <span class=c1># now upper triangle is 0s and only lower triangle is 1s</span>
</span></span><span class=line><span class=cl><span class=n>weights</span> <span class=o>=</span> <span class=n>weights</span> <span class=o>/</span> <span class=n>torch</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>weights</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># have each row sum up to 1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># average out tokens via matrix multiply</span>
</span></span><span class=line><span class=cl><span class=n>xbar2</span> <span class=o>=</span> <span class=n>weights</span> <span class=o>@</span> <span class=n>x</span>
</span></span></code></pre></div><p>Again, we start with the same dummy input where the sequence length is 8 and each token is in a 2D vector space. Then we instantiate the weights matrix $W$ of shape $T \times T$ where every value is 1.</p>$$
W =
\begin{bmatrix}
1 & 1 & 1 \\[3px]
1 & 1 & 1 \\[3px]
1 & 1 & 1
\end{bmatrix}
$$<p>We pass this matrix into <code>torch.tril()</code>, which is a nifty method returns the same matrix but the upper triangle is made all 0.</p>$$
W =
\begin{bmatrix}
1 & 0 & 0 \\[3px]
1 & 1 & 0 \\[3px]
1 & 1 & 1
\end{bmatrix}
$$<p>Then we normalize each row to sum up to 1 by finding the total sum along each row (<code>dim=1</code>) and dividing each element in that row by the sum.</p>$$
W =
\begin{bmatrix}
1 & 0 & 0 \\[3px]
\frac{1}{2} & \frac{1}{2} & 0 \\[3px]
\frac{1}{3} & \frac{1}{3} & \frac{1}{3}
\end{bmatrix}
$$<p>Finally, we take the average by executing the matrix multiplication between the weights and the inputs.</p><h2 id=adding-softmax>Adding softmax<a hidden class=anchor aria-hidden=true href=#adding-softmax>#</a></h2><p>Notice how in the last step we were normalizing the matrix so that each row sums up to 1. The keen reader will realize that we can use softmax to do that for us.</p><p>Right now, our matrix before the normalization looks like</p>$$
W =
\begin{bmatrix}
1 & 0 & 0 \\[3px]
1 & 1 & 0 \\[3px]
1 & 1 & 1
\end{bmatrix}
$$<p>For softmax to generate our desired normalized output, we need to input something like</p>$$
W =
\begin{bmatrix}
0 & -\infty & -\infty \\[3px]
0 & 0 & -\infty \\[3px]
0 & 0 & 0
\end{bmatrix}
$$<p>This is because softmax is defined as</p>$$
\text{softmax}(z)_i = \frac{e^{z_i}}{\sum_j e^{z_j}}.
$$<p>So if an entry is $-\infty$, then $e^{-\infty} = 0$, which means those positions get probability 0. And if all the unmasked entries are 0, softmax makes them uniform.</p><p>In code, this would look like</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>T</span><span class=p>,</span> <span class=n>C</span> <span class=o>=</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>2</span> <span class=c1># time, channels</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>T</span><span class=p>,</span> <span class=n>C</span><span class=p>)</span> <span class=c1># same dummy input</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tril</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>T</span><span class=p>,</span> <span class=n>T</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>T</span><span class=p>,</span><span class=n>T</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>weights</span> <span class=o>=</span> <span class=n>weights</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>tril</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;-inf&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;---&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>weights</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>weights</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=c1># normalize </span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>xbar3</span> <span class=o>=</span> <span class=n>weights</span> <span class=o>@</span> <span class=n>x</span>
</span></span></code></pre></div><pre tabindex=0><code>tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., -inf],
        [0., 0., 0., 0., 0., 0., 0., 0.]])
---
tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],
        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],
        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],
        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])
</code></pre><p>We create a <code>tril</code> matrix where the upper triangle is all zeros, and a <code>weights</code> matrix of the same shape but the entire thing is zeros. Then we lay the <code>tril</code> matrix on the <code>weights</code> one, and wherever <code>tril</code> is 0 (the upper triangle half), we change that spot in <code>weights</code> to $- \infty$. Then we apply softmax to the <code>weights</code> to normalize like before. And finally, we matrix multiply that with the input <code>x</code> to get our averaged scores of previous tokens.</p><h2 id=queries-keys-and-values>Queries, Keys, and Values<a hidden class=anchor aria-hidden=true href=#queries-keys-and-values>#</a></h2><p>Ok, this is good and all, but a token shouldn&rsquo;t have to give equal weightage to all of its previous tokens. It should be able to pay special attention (hint hint!) to certain tokens that have more relevant information to offer. For example, if I am a token &ldquo;dog&rdquo;, I should pay more attention to tokens that describe me like &ldquo;fluffy&rdquo; or &ldquo;running&rdquo; instead of other tokens that don&rsquo;t matter as much&ndash;it&rsquo;s data dependent.</p><p>How do we implement that? Imagine each token emits two vectors: a <strong>query</strong> and <strong>key</strong>. These are different metadata about itself. The query vector, roughly speaking, asks the question &ldquo;What am I looking for?&rdquo; The key vector says &ldquo;This is what information I contain.&rdquo;</p><p>Formally, each token has an embedding $x_t \in \R ^C$. We want to <strong>transform</strong> these vectors from dimension $C$ to another $d_k$. If you remember from linear algebra, that&rsquo;s exactly what a matrix does: transform a vector from one dimension to a vector in another dimension by linearly projecting it. So we learn two <strong>linear projections</strong></p>$$
W_Q \in \R^{C \times d_k}, \quad W_K \in \R^{C \times d_k}
$$<p>To get our query and key metadata for a token, we comput</p>$$
q_t = x_t W_Q, \quad k_t = x_t W_K
$$<p>where $q_t, k_t \in \R ^{d_k}$ (just fancy notation saying that the vectors are now in another dimension).</p><p>If we stack all token embeddings into a matrix</p>$$
X \in \R ^{T \times C},
$$<p>then we get</p>$$
Q = X W_Q, \quad K = X W_K \in \R ^ {T \times d_k}
$$<p>Implementing this in code is very easy. We can use <code>torch.nn.Linear()</code> to make the learnable projections:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>T</span><span class=p>,</span> <span class=n>C</span> <span class=o>=</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>32</span> <span class=c1># time, bigger channels now</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>T</span><span class=p>,</span> <span class=n>C</span><span class=p>)</span> <span class=c1># dummy input shape (T, C)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>head_size</span> <span class=o>=</span> <span class=mi>16</span> <span class=c1># this is our d_k </span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># these linear layers (simple matmuls) are what we use to emit the key and query vectors</span>
</span></span><span class=line><span class=cl><span class=n>key</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>C</span><span class=p>,</span> <span class=n>head_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>C</span><span class=p>,</span> <span class=n>head_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>k</span> <span class=o>=</span> <span class=n>key</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>   <span class=c1># shape (T, 16)</span>
</span></span><span class=line><span class=cl><span class=n>q</span> <span class=o>=</span> <span class=n>query</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=c1># shape (T, 16)</span>
</span></span></code></pre></div><p>Now to have each token communicate with each other, we take the dot product ($Q \cdot K)$. Each token&rsquo;s query vector will be multiplied by every other token&rsquo;s key vector. If a key and query vector are aligned, then their dot product will be very high and that&rsquo;s how we know they are related.</p><p>Formally, we compute the following <strong>attention score</strong> matrix</p>$$
S = QK ^\top \in \R ^{T \times T},
$$<p>where $S_{t, i} = q_t \cdot k_i$.</p><p>Row $t$ of $S$ contains how much token $t$ is interested in every other token. And this score matrix is what replaces our initial average score matrix! Now each weight can be custom tuned to how much a token should pay attention to other tokens.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Up to this point, NO communication has happened yet between the tokens. </span>
</span></span><span class=line><span class=cl><span class=c1># We&#39;ve only computed their key and query vectors. </span>
</span></span><span class=line><span class=cl><span class=c1># The dot product below is when the communication happens!</span>
</span></span><span class=line><span class=cl><span class=n>weights</span> <span class=o>=</span> <span class=n>q</span> <span class=o>@</span> <span class=n>k</span><span class=o>.</span><span class=n>T</span> <span class=c1># (T, 16) @ (16, T) -&gt; (T, T)</span>
</span></span></code></pre></div><p>From here, we take our scores matrix and apply softmax like before.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>tril</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>T</span><span class=p>,</span> <span class=n>T</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>weights</span> <span class=o>=</span> <span class=n>weights</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>tril</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;-inf&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>weights</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>weights</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><pre tabindex=0><code>tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],
        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],
        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],
        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]]
</code></pre><p>Notice how the weights are not even now across each row! Tokens are paying more attention to some tokens than others and giving them higher scores.</p><p>Finally, we don&rsquo;t aggregate across the raw input <code>x</code> like before. Instead, we calculate another vector from the input, called <strong>value</strong>. We aggregate across this. The value vector says, &ldquo;If you find me interesting (aka we have a high $Q \cdot K$ dot product), this is what I can communicate to you.&rdquo;</p><p>Formally, we learn a third projection</p>$$
W_V \in \R ^ {C \times d_v},
$$<p>and compute</p>$$
v_t = x_t W_v, \quad V=X W_V \in \R ^ {T \times d_v}
$$<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>value</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>C</span><span class=p>,</span> <span class=n>head_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span> <span class=c1># our d_v == d_k</span>
</span></span><span class=line><span class=cl><span class=n>v</span> <span class=o>=</span> <span class=n>value</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>out</span> <span class=o>=</span> <span class=n>weights</span> <span class=o>@</span> <span class=n>v</span>
</span></span></code></pre></div><h2 id=minor-notes>Minor notes<a hidden class=anchor aria-hidden=true href=#minor-notes>#</a></h2><p>A few things to note:</p><ul><li>Keys and queries are projected to $d_k$ and values are projected to $d_v$. In this case, $d_k = d_v$.</li></ul><ul><li>Attention is simply a <strong>communication</strong> mechanism. You can view it as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all its neighbors, where the weights are data dependent.</li></ul><ul><li>Attention simply aggregates over a set of vectors&ndash;there is no notion of space or position. This is why we need to add positional encodings to the vectors.</li></ul><ul><li>The <code>tril()</code> creates something called the causal mask, which prevents all tokens from seeing and communicating each other. Sometimes, this is important like in a task like sentiment classification. In this task, we need to see the entire sentence before we classify it as happy or sad. In such tasks, we can implement attention the same way, just without the mask.</li></ul><ul><li><strong>Self-attention</strong> just means that the keys and values come from the same source as the queries. <strong>Cross-attention</strong> means that the queries still get produced from input <code>x</code>, but the keys and values come from somewhere else (like an encoder block).</li></ul><ul><li>Lastly, we can&rsquo;t just use <code>weights</code> as is. Remember, <code>weights</code> is calculated by taking the dot product between $K$ and $Q$. Sometimes, these dot products can be really high and cause numerical instability. So, we need to control the variance of the matrix by <strong>scaling by $\frac{1}{\sqrt{d_k}}$</strong>. This way, the softmax output will remain fairly diffused and not too saturated (think of a really peaky probability distribution versus a fairly smooth one).</li></ul><h2 id=fin>Fin<a hidden class=anchor aria-hidden=true href=#fin>#</a></h2><p>Finally, we have implemented the simplest form of attention: scaled dot product attention (SDPA for short)!</p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right)V
$$<p>In the next blog post, we will see how the attention operation fits into a bigger model architecture called the transformer. We will compare the more powerful transformer against our humble bigram model and measure the difference in performance. In the end, we will actually have a model that can decently generate text!</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/my-first-post/><span class=title>Next »</span><br><span>My First Post</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Atten-hut! on x" href="https://x.com/intent/tweet/?text=Atten-hut%21&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fattention%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Atten-hut! on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fattention%2f&amp;title=Atten-hut%21&amp;summary=Atten-hut%21&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fattention%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Atten-hut! on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fattention%2f&title=Atten-hut%21"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Atten-hut! on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fattention%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Atten-hut! on whatsapp" href="https://api.whatsapp.com/send?text=Atten-hut%21%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fattention%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Atten-hut! on telegram" href="https://telegram.me/share/url?text=Atten-hut%21&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fattention%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Atten-hut! on ycombinator" href="https://news.ycombinator.com/submitlink?t=Atten-hut%21&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fattention%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=http://localhost:1313/>Arav Tewari's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>