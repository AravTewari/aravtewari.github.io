[{"content":"In this blog, I want to build attention from scratch. But I don\u0026rsquo;t want to jump straight into the best implementation. We\u0026rsquo;re going to start with dumbest, most naive possible code, see why it fails, and then add one idea at a time until attention falls on your head like the apple fell on Newton\u0026rsquo;s.\nI learned it this way and it really stuck to me. This is heavily inspired from Karpathy\u0026rsquo;s video, and this concept has probably been beaten to hell. But the point of this blog is to just practice formalizing my notes.\nThe task: next character prediction The setup is: given a set of characters, how can we predict the next best character? Models like ChatGPT don\u0026rsquo;t actually work at the character level, they work at something called the token level. This is a blog for later, but tokens are just parts of a word. So a word like \u0026ldquo;homework\u0026rdquo; will be broken into 2 tokens \u0026ldquo;home-work\u0026rdquo;. For now, our tokens are characters. The set of unique tokens is called our vocabulary, and it is of size $V$.\nSo, we take long string (more formally called a sequence) of text, convert it to integers, and try to train a model that predicts the next character given the previous ones.\nIf the sequence of tokens is $$ (x_1, x_2, \\dots, x_T) $$ then at position $t$, the model sees $x_1, x_2, \\dots, x_t$ and is trained to predict $x_{t+1}$. This can be written as: $$ p(x_{t+1} \\mid x_1, \\dots, x_t) $$What\u0026rsquo;s important to note here is how many tokens can the model see into the past? How big should $t$ be? That\u0026rsquo;s called context. The code below clearly visualizes that idea.\nblock_size = 8 x = train_data[:block_size] y = train_data[1:block_size + 1] for t in range(block_size): context = x[:t + 1] target = y[t] print(f\u0026#39;When context is {context}, target is {target}\u0026#39;) When context is [18], target is 47 When context is [18, 47], target is 56 When context is [18, 47, 56], target is 57 When context is [18, 47, 56, 57], target is 58 When context is [18, 47, 56, 57, 58], target is 1 When context is [18, 47, 56, 57, 58, 1], target is 15 When context is [18, 47, 56, 57, 58, 1, 15], target is 47 When context is [18, 47, 56, 57, 58, 1, 15, 47], target is 58 block_size is how big our context is. We look 8 tokens into the past to predict the next one. x (our input) is the first 8 samples in our train set. y (our output or target) is also 8 samples long, but it\u0026rsquo;s offset by 1. Why the offset? It\u0026rsquo;s easier to answer that with an example.\nAt timestep 0, we want to use the all tokens up to token 0 (so x = [token_0]) to predict y = token_1.\nAt timestep 1, we we want to use all tokens up to token 1 (so x = [token_0, token_1]) to predict y = token_2.\nAt timestep 3, we want to use all tokens up to token 2 (so x = [token_0, token_1, token_2]) to predict the y = token_3.\nExtend this to some timestep t, we get \u0026ldquo;we want to use all tokens up to token t to predict the t+1 token. And if we make y by an offset by one, token t+1 is stored at index t. Now the code should make sense!\nThe simplest possible model Now that we understand what we are training our model to do, let\u0026rsquo;s start with the easiest implementation. This would be to just ignore the context and only look at the last token when predicting the current one. This is called a bigram language model.\nThe way we implement this is a lookup table. We should be able to look up the current token and get scores for the next token, and we choose the token with the highest score. So this table should have $V$ spots (remember $V$ is the size of our vocabulary or the number of unique tokens). And in each spot, we should have a score for all of the tokens, which means we should have $V$ scores. This means our table will be size $V \\times V$.\nWe can code this out as follows.\ntoken_table = nn.Embedding(vocab_size, vocab_size) This creates a matrix: $$ W \\in \\R ^ {V \\times V} $$ Given an input token $x_t \\in \\{0, \\dots, V-1 \\}$, the table returns logits $$ \\ell _t = W[x_t] \\in \\R^V $$ The logits are our scores for each token. We can convert these scores into a probability distribution, and then sample from that. That will give us our next, most likely token.\nThe way we do that is by passing the logits through a function called the softmax. This will take our raw scores and scale each value to be between 0 and 1 and make sure all the values sum up to 1. This is the requirement for a probability distribution. Finally, we sample from this distribution to get our next token.\nIn PyTorch, this is coded as:\nclass BigramModel(nn.Module): def __init__(self, vocab_size): self.token_table = nn.Embedding(vocab_size, vocab_size) def forward(self, current_token): next_token_logits = self.token_table(current_token) return next_token_logits def generate(self, current_token, number_of_new_tokens): result = [] for _ in range(number_of_new_tokens): # make prediction next_token_logits = self.forward(current_token) # convert from probability distribution next_token_dist = F.softmax(next_token_logits, dim=-1) # sample from distribution next_token = torch.multinomial(next_token_dist, num_samples=1) # append next token to result result.append(next_token) # move onto next time step curr_token = next_token return result Right now, this simple bigram model doesn\u0026rsquo;t use any of the previous tokens to inform its predictions. Formally, we are learning $$ p(x_{t+1} \\mid x_t) $$ but our task requires learning $$ p(x_{t+1} \\mid x_1, \\dots, x_t) $$The mathematical trick in attention How can we use the information from our context. Well, the first thing we need to do is assume our tokens contain information. Instead of simple scalars, we can say our tokens are vectors of size $C$, which stands for channels. These fancier tokens are called token embeddings. So suppose our scalar tokens $x_1, x_2, \\dots x_T \\in \\R$ have been embedded into a vector space and are now $$ x_1, x_2, \\dots, x_T \\in \\R ^C $$The simplest way we can use our context is by looking at the previous tokens and take their average, which would represent some sort of feature vector. Note that taking the average is pretty lossy because we lose quite a bit of information (like each tokens position and we treat all tokens equally). We will see on how to recover that later. So we want $$ \\bar{x}_t = \\frac{1}{t} \\sum_{i=1}^t x_i $$ This is pretty simple to code out. Assume that we have a sequence of 8 tokens and each token is in a 2D vector space:\nT, C = 8, 2 # time, channels x = torch.randn(T, C) # dummy input x_bar = torch.zeros((T, C)) for t in range(T): prev_context = x[:t+1] # get all tokens up to timestep t x_bar[t] = torch.mean(prev_context, 0) # take the average print(x_bar[0] == x[0]) # true, because the average only consists of the first token print(x_bar[1] == x[1]) # false, because the average now consists of the first 2 tokens Finding the average from a for loop is a pretty slow operation. Let\u0026rsquo;s see how to speed this up.\nWe can think of the average as a weighted sum. For example, if we are taking the average of 4 inputs, then we are multiplying each input by 0.25 and adding them up. This is literally the summation expanded out $$ \\bar{x}_4 = \\frac{1}{4} \\sum_{i=1}^4 x_i \\\\ \\bar{x}_4 = \\frac{1}{4} (x_1 + x_2 + x_3 + x_4) \\\\ \\bar{x}_4 = \\frac{1}{4} x_1 + \\frac{1}{4} x_2 + \\frac{1}{4} x_3 + \\frac{1}{4} x_4 $$We can express this weighted sum as a dot product between two vectors: the inputs and their respective weights. Suppose each $x_i \\in \\R^C$ is a vector (not a scalar). Then the average at timestep 4 can be written as $$ \\bar{x}_4 = \\begin{bmatrix} \\frac{1}{4} \u0026 \\frac{1}{4} \u0026 \\frac{1}{4} \u0026 \\frac{1}{4} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{bmatrix} $$The first vector is our weights and the second transposed vector is our input.\nNow remember in each time step, we have one more token added to our context when predicting the next target token.\nt=0: When context is [x1], target is x2 t=1: When context is [x1, x2], target is x3 t=2: When context is [x1, x2, x3], target is x4 At timestep $t = 0$, the context is just $[x_1]$, so the average is $$ \\bar{x}_1 = 1 \\cdot x_1 $$At timestep $t=1$, the context is $[x_1, x_2]$, so the average is $$ \\bar{x}_2 = \\frac{1}{2} x_1 + \\frac{1}{2} x_2 $$At timestep $t=2$, the context is $[x_1, x_2, x_3]$, so the average is $$ \\bar{x}_3 = \\frac{1}{3} x_1 + \\frac{1}{3} x_2 + \\frac{1}{3} x_3 $$And remember how we just reformulated the average into a dot product between the inputs and the weights? The weights at each timstep would clearly look like $$ [1] $$ $$ [\\tfrac{1}{2}, \\, \\tfrac{1}{2}] $$ $$ \\left[\\tfrac{1}{3}, \\, \\tfrac{1}{3}, \\, \\tfrac{1}{3}\\right] $$Now with these two sets of input and weight vectors, we can arrange them into matrices. Let the input matrix be $$ X = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\in \\mathbb{R}^{3 \\times C}. $$And let the weight matrix be $$ W = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\[3px] \\frac{1}{2} \u0026 \\frac{1}{2} \u0026 0 \\\\[3px] \\frac{1}{3} \u0026 \\frac{1}{3} \u0026 \\frac{1}{3} \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 3}. $$Notice that $W$ is lower triangular. That is important. It ensures that at timestep $t$, only tokens up to $t$ are used in the average If we are predicting token 5, we should only be averaging tokens 1-4 from the data. Using information from the actual token 5 in the data or tokens 6, 7, $\\dots T$ is cheating! The zeros prevent any future token from leaking into the computation. This is called the causal mask.\nAnd to perform the dot product and get the averaged out tokens, we just do a matrix multiplication: $$ \\bar{X} = W X \\in \\R^{3 \\times C} $$The result contains $$ \\bar{X} = \\begin{bmatrix} \\bar{x}_1 \\\\ \\bar{x}_2 \\\\ \\bar{x}_3 \\end{bmatrix} $$Each row of $\\bar{X}$ is the average of all tokens up to that timestep.\nThat\u0026rsquo;s the mathematical trick in attention! Instead of writing for-loops that repeatedly sum over previous tokens, the entire operation can be expressed as a single matrix multiplication that acts as a dot product between weights and the inputs.\nThis can be coded very easily\nT, C = 8, 2 # time, channels x = torch.randn(T, C) # same dummy input # create weights matrix for weighted sum weights = torch.ones(T, T) # all 1s matrix weights = torch.tril(weights) # now upper triangle is 0s and only lower triangle is 1s weights = weights / torch.sum(weights, dim=1, keepdims=True) # have each row sum up to 1 # average out tokens via matrix multiply xbar2 = weights @ x Again, we start with the same dummy input where the sequence length is 8 and each token is in a 2D vector space. Then we instantiate the weights matrix $W$ of shape $T \\times T$ where every value is 1. $$ W = \\begin{bmatrix} 1 \u0026 1 \u0026 1 \\\\[3px] 1 \u0026 1 \u0026 1 \\\\[3px] 1 \u0026 1 \u0026 1 \\end{bmatrix} $$We pass this matrix into torch.tril(), which is a nifty method returns the same matrix but the upper triangle is made all 0. $$ W = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\[3px] 1 \u0026 1 \u0026 0 \\\\[3px] 1 \u0026 1 \u0026 1 \\end{bmatrix} $$Then we normalize each row to sum up to 1 by finding the total sum along each row (dim=1) and dividing each element in that row by the sum. $$ W = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\[3px] \\frac{1}{2} \u0026 \\frac{1}{2} \u0026 0 \\\\[3px] \\frac{1}{3} \u0026 \\frac{1}{3} \u0026 \\frac{1}{3} \\end{bmatrix} $$Finally, we take the average by executing the matrix multiplication between the weights and the inputs.\nAdding softmax Notice how in the last step we were normalizing the matrix so that each row sums up to 1. The keen reader will realize that we can use softmax to do that for us.\nRight now, our matrix before the normalization looks like $$ W = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\[3px] 1 \u0026 1 \u0026 0 \\\\[3px] 1 \u0026 1 \u0026 1 \\end{bmatrix} $$ For softmax to generate our desired normalized output, we need to input something like $$ W = \\begin{bmatrix} 0 \u0026 -\\infty \u0026 -\\infty \\\\[3px] 0 \u0026 0 \u0026 -\\infty \\\\[3px] 0 \u0026 0 \u0026 0 \\end{bmatrix} $$This is because softmax is defined as $$ \\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}. $$ So if an entry is $-\\infty$, then $e^{-\\infty} = 0$, which means those positions get probability 0. And if all the unmasked entries are 0, softmax makes them uniform.\nIn code, this would look like\nT, C = 8, 2 # time, channels x = torch.randn(T, C) # same dummy input tril = torch.tril(torch.ones(T, T)) weights = torch.zeros((T,T)) weights = weights.masked_fill(tril == 0, float(\u0026#39;-inf\u0026#39;)) print(weights) print(\u0026#34;---\u0026#34;) weights = F.softmax(weights, dim=1) # normalize print(weights) xbar3 = weights @ x tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf], [0., 0., -inf, -inf, -inf, -inf, -inf, -inf], [0., 0., 0., -inf, -inf, -inf, -inf, -inf], [0., 0., 0., 0., -inf, -inf, -inf, -inf], [0., 0., 0., 0., 0., -inf, -inf, -inf], [0., 0., 0., 0., 0., 0., -inf, -inf], [0., 0., 0., 0., 0., 0., 0., -inf], [0., 0., 0., 0., 0., 0., 0., 0.]]) --- tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000], [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000], [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000], [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000], [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]]) We create a tril matrix where the upper triangle is all zeros, and a weights matrix of the same shape but the entire thing is zeros. Then we lay the tril matrix on the weights one, and wherever tril is 0 (the upper triangle half), we change that spot in weights to $- \\infty$. Then we apply softmax to the weights to normalize like before. And finally, we matrix multiply that with the input x to get our averaged scores of previous tokens.\nQueries, Keys, and Values Ok, this is good and all, but a token shouldn\u0026rsquo;t have to give equal weightage to all of its previous tokens. It should be able to pay special attention (hint hint!) to certain tokens that have more relevant information to offer. For example, if I am a token \u0026ldquo;dog\u0026rdquo;, I should pay more attention to tokens that describe me like \u0026ldquo;fluffy\u0026rdquo; or \u0026ldquo;running\u0026rdquo; instead of other tokens that don\u0026rsquo;t matter as much\u0026ndash;it\u0026rsquo;s data dependent.\nHow do we implement that? Imagine each token emits two vectors: a query and key. These are different metadata about itself. The query vector, roughly speaking, asks the question \u0026ldquo;What am I looking for?\u0026rdquo; The key vector says \u0026ldquo;This is what information I contain.\u0026rdquo;\nFormally, each token has an embedding $x_t \\in \\R ^C$. We want to transform these vectors from dimension $C$ to another $d_k$. If you remember from linear algebra, that\u0026rsquo;s exactly what a matrix does: transform a vector from one dimension to a vector in another dimension by linearly projecting it. So we learn two linear projections $$ W_Q \\in \\R^{C \\times d_k}, \\quad W_K \\in \\R^{C \\times d_k} $$To get our query and key metadata for a token, we comput $$ q_t = x_t W_Q, \\quad k_t = x_t W_K $$ where $q_t, k_t \\in \\R ^{d_k}$ (just fancy notation saying that the vectors are now in another dimension).\nIf we stack all token embeddings into a matrix $$ X \\in \\R ^{T \\times C}, $$ then we get $$ Q = X W_Q, \\quad K = X W_K \\in \\R ^ {T \\times d_k} $$Implementing this in code is very easy. We can use torch.nn.Linear() to make the learnable projections:\nT, C = 8, 32 # time, bigger channels now x = torch.randn(T, C) # dummy input shape (T, C) head_size = 16 # this is our d_k # these linear layers (simple matmuls) are what we use to emit the key and query vectors key = nn.Linear(C, head_size, bias=False) query = nn.Linear(C, head_size, bias=False) k = key(x) # shape (T, 16) q = query(x) # shape (T, 16) Now to have each token communicate with each other, we take the dot product ($Q \\cdot K)$. Each token\u0026rsquo;s query vector will be multiplied by every other token\u0026rsquo;s key vector. If a key and query vector are aligned, then their dot product will be very high and that\u0026rsquo;s how we know they are related.\nFormally, we compute the following attention score matrix $$ S = QK ^\\top \\in \\R ^{T \\times T}, $$ where $S_{t, i} = q_t \\cdot k_i$.\nRow $t$ of $S$ contains how much token $t$ is interested in every other token. And this score matrix is what replaces our initial average score matrix! Now each weight can be custom tuned to how much a token should pay attention to other tokens.\n# Up to this point, NO communication has happened yet between the tokens. # We\u0026#39;ve only computed their key and query vectors. # The dot product below is when the communication happens! weights = q @ k.T # (T, 16) @ (16, T) -\u0026gt; (T, T) From here, we take our scores matrix and apply softmax like before.\ntril = torch.tril(torch.ones(T, T)) weights = weights.masked_fill(tril == 0, float(\u0026#39;-inf\u0026#39;)) weights = F.softmax(weights, dim=1) tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000], [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000], [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000], [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000], [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]] Notice how the weights are not even now across each row! Tokens are paying more attention to some tokens than others and giving them higher scores.\nFinally, we don\u0026rsquo;t aggregate across the raw input x like before. Instead, we calculate another vector from the input, called value. We aggregate across this. The value vector says, \u0026ldquo;If you find me interesting (aka we have a high $Q \\cdot K$ dot product), this is what I can communicate to you.\u0026rdquo;\nFormally, we learn a third projection $$ W_V \\in \\R ^ {C \\times d_v}, $$ and compute $$ v_t = x_t W_v, \\quad V=X W_V \\in \\R ^ {T \\times d_v} $$value = nn.Linear(C, head_size, bias=False) # our d_v == d_k v = value(x) out = weights @ v Minor notes A few things to note:\nKeys and queries are projected to $d_k$ and values are projected to $d_v$. In this case, $d_k = d_v$. Attention is simply a communication mechanism. You can view it as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all its neighbors, where the weights are data dependent. Attention simply aggregates over a set of vectors\u0026ndash;there is no notion of space or position. This is why we need to add positional encodings to the vectors. The tril() creates something called the causal mask, which prevents all tokens from seeing and communicating each other. Sometimes, this is important like in a task like sentiment classification. In this task, we need to see the entire sentence before we classify it as happy or sad. In such tasks, we can implement attention the same way, just without the mask. Self-attention just means that the keys and values come from the same source as the queries. Cross-attention means that the queries still get produced from input x, but the keys and values come from somewhere else (like an encoder block). Lastly, we can\u0026rsquo;t just use weights as is. Remember, weights is calculated by taking the dot product between $K$ and $Q$. Sometimes, these dot products can be really high and cause numerical instability. So, we need to control the variance of the matrix by scaling by $\\frac{1}{\\sqrt{d_k}}$. This way, the softmax output will remain fairly diffused and not too saturated (think of a really peaky probability distribution versus a fairly smooth one). Fin Finally, we have implemented the simplest form of attention: scaled dot product attention (SDPA for short)! $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right)V $$In the next blog post, we will see how the attention operation fits into a bigger model architecture called the transformer. We will compare the more powerful transformer against our humble bigram model and measure the difference in performance. In the end, we will actually have a model that can decently generate text!\n","permalink":"http://localhost:1313/posts/attention/","summary":"\u003cp\u003eIn this blog, I want to build attention from scratch. But I don\u0026rsquo;t want to jump straight into the best implementation. We\u0026rsquo;re going to start with dumbest, most naive possible code, see why it fails, and then add one idea at a time until attention falls on your head like the apple fell on Newton\u0026rsquo;s.\u003c/p\u003e\n\u003cp\u003eI learned it this way and it really stuck to me. This is heavily inspired from Karpathy\u0026rsquo;s video, and this concept has probably been beaten to hell. But the point of this blog is to just practice formalizing my notes.\u003c/p\u003e","title":"Atten-hut!"},{"content":"What is this site for? This will just be an online notebook where I will jot down my learnings. It is mainly for me to formalize my own thoughts. If I think my notes are quality enough, then I will post them here. This serves to help me rather than spread new knowledge, but who knows, maybe one day I\u0026rsquo;ll share something groundbreaking here ¯\\_(ツ)_/¯.\nWhat will I post here? I will start off with putting my notes down about attention, the transformer, and GPT-2. Then I will add a post about the Llama-2 model. I hope to also make clean notes for the class I am taking right now, 18789: Deep Generative Modeling. I feel the professors teach very intutively, and if I were to scribe down lectures, I would learn a lot more than just sitting in class and paying attention.\nBut other than that, we\u0026rsquo;ll see where my studies take me!\n","permalink":"http://localhost:1313/posts/my-first-post/","summary":"\u003ch2 id=\"what-is-this-site-for\"\u003eWhat is this site for?\u003c/h2\u003e\n\u003cp\u003eThis will just be an online notebook where I will jot down my learnings. It is mainly for me to formalize my own thoughts. If I think my notes are quality enough, then I will post them here. This serves to help me rather than spread new knowledge, but who knows, maybe one day I\u0026rsquo;ll share something groundbreaking here \u003ccode\u003e¯\\_(ツ)_/¯\u003c/code\u003e.\u003c/p\u003e\n\u003ch2 id=\"what-will-i-post-here\"\u003eWhat will I post here?\u003c/h2\u003e\n\u003cp\u003eI will start off with putting my notes down about attention, the transformer, and GPT-2. Then I will add a post about the Llama-2 model. I hope to also make clean notes for the class I am taking right now, \u003cem\u003e18789: Deep Generative Modeling\u003c/em\u003e. I feel the professors teach very intutively, and if I were to scribe down lectures, I would learn a lot more than just sitting in class and paying attention.\u003c/p\u003e","title":"My First Post"}]